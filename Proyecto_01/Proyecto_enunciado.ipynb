{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.dii.uchile.cl/wp-content/uploads/2021/06/Magi%CC%81ster-en-Ciencia-de-Datos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Proyecto 1 - MDS7202 Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos üìö**\n",
    "\n",
    "**MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos**\n",
    "\n",
    "### Cuerpo Docente:\n",
    "\n",
    "- Profesor: Ignacio Meza, Gabriel Iturra\n",
    "- Auxiliar: Sebasti√°n Tinoco\n",
    "- Ayudante: Arturo Lazcano, Angelo Mu√±oz\n",
    "\n",
    "*Por favor, lean detalladamente las instrucciones de la tarea antes de empezar a escribir.*\n",
    "\n",
    "### Equipo:\n",
    "\n",
    "- Jorge Allendes\n",
    "- Valentina Castro\n",
    "\n",
    "\n",
    "### Link de repositorio de GitHub: `https://github.com/J07Aa/Repo`\n",
    "\n",
    "Fecha l√≠mite de entrega üìÜ: 27 de Octubre de 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Reglas\n",
    "\n",
    "- **Grupos de 2 personas.**\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
    "- Estrictamente prohibida la copia. \n",
    "- Pueden usar cualquier material del curso que estimen conveniente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://worldskateamerica.org/wp-content/uploads/2023/07/SANTIAGO-2023-1-768x153.jpg\" alt=\"Descripci√≥n de la imagen\">\n",
    "</div>\n",
    "\n",
    "En un Chile azotado por un profundo caos pol√≠tico-econ√≥mico y el resurgimiento de programas de televisi√≥n de dudosa calidad, todas las miradas y esperanzas son depositadas en el √©xito de un √∫nico evento: Santiago 2023. La naci√≥n necesitaba desesperadamente un respiro, y los Juegos de Santiago 2023 promet√≠an ser una luz al final del t√∫nel.\n",
    "\n",
    "El Presidente de la Rep√∫blica -conocido en las calles como Bomb√≠n-, consciente de la importancia de este evento para la revitalizaci√≥n del pa√≠s, decide convocar a usted y su equipo en calidad de expertos en an√°lisis de datos y estad√≠sticas. Con gran solemnidad, el presidente les encomienda una importante y peligrosa: liderar un proyecto que permitiera caracterizar de forma autom√°tica y eficiente los datos generados por estos magnos juegos. Para esto, el presidente le destaca que la soluci√≥n debe considerar los siguientes puntos:\n",
    "- Caracterizaci√≥n autom√°tica de los datos\n",
    "- La soluci√≥n debe ser compatible con cualquier dataset\n",
    "- Se les facilita el dataset *olimpiadas.parquet*, el cual recopila data de diferentes juegos ol√≠mpicos realizados en los √∫ltimos a√±os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Creaci√≥n de `Profiler` Class (4.0 puntos)\n",
    "\n",
    "Cree la clase `Profiler`. Como m√≠nimo, esta debe tener las siguientes funcionalidades:\n",
    "\n",
    "1. El m√©todo constructor, el cual debe recibir los datos a procesar en formato `Pandas DataFrame`. Adem√°s, este m√©todo debe generar una carpeta en su directorio de trabajo con el nombre `EDA_fecha`, donde `fecha` corresponda a la fecha de ejecuci√≥n en formato `DD-MM-YYYY`.\n",
    "\n",
    "2. El m√©todo `summarize`, el cual debe caracterizar las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    - Reportar el tipo de variable\n",
    "    - Reportar el n√∫mero y/o porcentaje de valores √∫nicos de la variable\n",
    "    - Reportar el n√∫mero y/o porcentaje de valores nulos\n",
    "    - Si la variables es num√©rica:\n",
    "        - Reportar el n√∫mero y/o porcentaje de valores cero, negativos y outliers\n",
    "        - Reportar estad√≠stica descriptiva como el valor m√≠nimo, m√°ximo, promedio y los percentiles 25, 50, 75 y 100\n",
    "   - Levantar una alerta en caso de encontrar alguna anomal√≠a fuera de lo com√∫n (el criterio debe ser ajustable por el usuario)\n",
    "   - Guardar sus resultados en el directorio `EDA_fecha/summary.txt`. El archivo debe separar de forma clara y ordenada los resultados de cada punto.\n",
    "\n",
    "3. El m√©todo `plot_vars`, el cual debe graficar la distribuci√≥n e interraciones de las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/plots`\n",
    "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    - Para las variables num√©ricas:\n",
    "        - Genere un gr√°fico de distribuci√≥n de densidad\n",
    "        - Grafique la correlaci√≥n entre las variables\n",
    "    - Para las variables categ√≥ricas:\n",
    "        - Genere un histograma de las top N categor√≠as (N debe ser un par√°metro ajustable)\n",
    "        - Grafique el coeficiente V de Cramer entre las variables\n",
    "    - Guardar cada gr√°fico generado en la carpeta `EDA_fecha/plots` en formato `.pdf` y bajo el naming `variable.pdf`, donde `variable` es el nombre de la variable de inter√©s\n",
    "    \n",
    "4. El m√©todo `clean_data`, el cual debe limpiar los datos para que luego puedan ser procesados. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/clean_data`\n",
    "    - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    - Drop de valores duplicados\n",
    "    - Implementar como m√≠nimo 2 t√©cnicas para tratar los valores nulos, como:\n",
    "        - Drop de valores nulos\n",
    "        - Imputar valores nulos con alguna t√©cnica de imputaci√≥n\n",
    "        - Funcionalidad para escoger entre una t√©cnica y la otra.\n",
    "    - Una de las columnas del dataframe presenta datos *no at√≥micos*. Separe dicha columna en las columnas que la compongan.\n",
    "        - Hint: ¬øQu√© caracteres permiten separar una columna de otra?\n",
    "        - Para las pruebas con el dataset nuevo, puede esperar que exista al menos una columna con este tipo de problema. Asuma que los separadores ser√°n los mismos, aunque el n√∫mero de columnas a separar puede ser distinto.\n",
    "    - Deber√≠an usar `FunctionTransformer`.\n",
    "    - Guardar los datos procesados en formato `.csv` en el path `EDA_fecha/clean_data/data.csv`\n",
    "\n",
    "5. El m√©todo `scale`, el cual debe preparar adecuadamente los datos para luego ser consumidos por alg√∫n tipo de algoritmo. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/scale`\n",
    "    - Procesar de forma adecuada los datos num√©ricos y categ√≥ricos:\n",
    "        - Su m√©todo debe recibir las t√©cnicas de escalamiento como argumento de entrada (utilizar solo t√©cnicas compatibles con el framework de `sklearn`)\n",
    "        - Para los atributos num√©ricos, se transforme los datos con un escalador logar√≠tmico y un `MinMaxScaler`\n",
    "        - Asuma que no existen datos ordinales en su dataset\n",
    "    - Guardar todo este procesamiento en un `ColumnTransformer`.\n",
    "    - Guardar los datos limpios y transformados en formato `.csv` en el path `EDA_fecha/process/scaled_features.csv`\n",
    "\n",
    "6. El m√©todo `make_clusters`, el cual debe generar clusters de los datos usando alg√∫n algoritmo de clusterizaci√≥n. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    - Crear la carpeta `EDA_fecha/clusters`\n",
    "    - Generar un estudio del codo donde se√±ale la cantidad de clusters optimos para el desarrollo.\n",
    "    - Su m√©todo debe recibir el algoritmo de clustering como argumento de entrada (utilizar solo algoritmos compatibles con el framework de `sklearn`).\n",
    "    - No olvide pre procesar adecuadamente los datos antes de implementar la t√©cnica de clustering. \n",
    "    - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5. \n",
    "    - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
    "    - Una vez generado los clusters, proyecte los datos a 2 dimensiones usando su t√©cnica de reducci√≥n de dimensionalidad favorita y grafique los resultados coloreando por cluster.\n",
    "    - Guardar los datos con su respectivo cluster en formato `.csv` en el path `EDA_fecha/clusters/data_clusters.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
    "\n",
    "7. El m√©todo `detect_anomalies`, el cual debe detectar anomal√≠as en los datos. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "\n",
    "    - Crear la carpeta `EDA_fecha/anomalies`\n",
    "    - Implementar alguna t√©cnica de detecci√≥n de anomal√≠as.\n",
    "    - Al igual que el punto anterior, su m√©todo debe considerar los siguientes puntos:\n",
    "        - No olvide pre procesar de forma adecuada los datos antes de implementar la t√©cnica de detecci√≥n de anomal√≠a. \n",
    "        - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5. \n",
    "        - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
    "        - Su m√©todo debe recibir el algoritmo como argumento de entrada\n",
    "        - Una vez generado las etiquetas, proyecte los datos a 2 dimensiones y grafique los resultados coloreando por las etiquetas predichas por el detector de anomal√≠as\n",
    "    - Guardar los datos con su respectiva etiqueta en formato `.csv` en el path `EDA_fecha/anomalies/data_anomalies.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
    "\n",
    "8. El m√©todo `profile`, el cual debe ejecutar todos los m√©todos anteriores.\n",
    "\n",
    "9. Crear el m√©todo `clearGarbage` para eliminar las carpetas/archivos creados/as por la clase `Profiler`.\n",
    "\n",
    "Algunas consideraciones generales:\n",
    "- Su clase ser√° testeada con datos tabulares diferentes a los provistos. No desarrollen c√≥digo *hardcodeado*: su clase debe ser capaz de funcionar para **cualquier** dataset. \n",
    "- Aplique todo su conocimiento sobre buenas pr√°cticas de programaci√≥n: se evaluar√° que su c√≥digo sea limpio y ordenado.\n",
    "- Recuerden documentar cada una de las funcionalidades que implementen.\n",
    "- Recuerden adjuntar sus `requirements.txt` junto a su entrega de proyecto. **El c√≥digo que no se pueda ejecutar por imcompatibilidades de librer√≠as no ser√° corregido.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jorgeallendes/anaconda3/envs/mds7202/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import librerias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from pandas.api.types import is_object_dtype\n",
    "from pandas.core.dtypes.common import is_datetime_or_timedelta_dtype\n",
    "from scipy.stats import chi2_contingency\n",
    "import scipy.stats as ss\n",
    "from scipy.stats.contingency import association\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import FunctionTransformer, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import OneClassSVM\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones auxiliares\n",
    "def cramers_corrected_stat(confusion_matrix):\n",
    "    \"\"\" calculate Cramers V statistic for categorical-categorical association.\n",
    "        uses correction from Bergsma and Wicher, \n",
    "        Journal of the Korean Statistical Society 42 (2013): 323-328\n",
    "    \"\"\"\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))    \n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))\n",
    "def column_spliter(df, columns_to_split, type_data_split):\n",
    "                \n",
    "    for col_to_split in columns_to_split:\n",
    "        data_aux = {'Texto': [col_to_split]}\n",
    "        df_aux = pd.DataFrame(data_aux)\n",
    "        # Dividir la columna 'Texto' en m√∫ltiples columnas utilizando la expresiones regulares\n",
    "        df_aux = df_aux['Texto'].str.split(r'[\\|*\\¬°?¬ø\\_\\-\\;\\{\\}\\(\\):,\\!\\+\\&]',expand = True)\n",
    "        col_parts = df_aux.loc[0,:].to_list()\n",
    "        df.loc[:,col_parts] = df[col_to_split].str.split(r'[\\|*\\¬°?¬ø\\_\\-\\;\\{\\}\\(\\):,\\!\\+\\&]', expand=True).to_numpy()\n",
    "        if col_to_split in type_data_split:\n",
    "            for col in col_parts:\n",
    "                df[col] = df[col].astype(type_data_split[col_to_split])\n",
    "        df = df.drop([col_to_split], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Profiler():\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.df = data\n",
    "        self.date = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "        self.output_folder = f\"EDA_{self.date}\"\n",
    "        self.data_clean = None\n",
    "        self.scaled_data = None\n",
    "        self.clean_pipeline = None\n",
    "        self.scale_transform = None\n",
    "        self.clustering_pipe = None\n",
    "        self.anomaly_pipe = None\n",
    "        # Se crea la carpeta (asumimos que no existe)\n",
    "        os.mkdir(self.output_folder)\n",
    "\n",
    "    # El m√©todo `summarize`, el cual debe caracterizar las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    # - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    # - Reportar el tipo de variable\n",
    "    # - Reportar el n√∫mero y/o porcentaje de valores √∫nicos de la variable\n",
    "    # - Reportar el n√∫mero y/o porcentaje de valores nulos\n",
    "    # - Si la variables es num√©rica:\n",
    "    #     - Reportar el n√∫mero y/o porcentaje de valores cero, negativos y outliers\n",
    "    #    - Reportar estad√≠stica descriptiva como el valor m√≠nimo, m√°ximo, promedio y los percentiles 25, 50, 75 y 100\n",
    "    # - Levantar una alerta en caso de encontrar alguna anomal√≠a fuera de lo com√∫n (el criterio debe ser ajustable por el usuario)\n",
    "    # - Guardar sus resultados en el directorio `EDA_fecha/summary.txt`. El archivo debe separar de forma clara y ordenada los resultados de cada punto.\n",
    "\n",
    "    # summarize: list, float -> None\n",
    "    # Esta funci√≥n se encarga de caracterizar todas las variables del dataset (l = None) o las variables especificadas \n",
    "    # en l y realizaremos una detecci√≥n de anomal√≠as en funci√≥n de los quintiles para cada caso de las variables num√©ricas\n",
    "    # utilizando un threshold dado por el usuario\n",
    "    def summarize(self, l=None, threshold=None,clean_data = False):\n",
    "        \"\"\" La funci√≥n pertenece a la clase Profilery se encarga de caracterizar el dataframe que contiene la \n",
    "        clase Profile, con la variable \"l\" se puede elegir las columnas que se analizar√°n, threshold es un numero \n",
    "        entero que se usa para caracterizar las anomalias este indica que quintiles ser√°n considerados anomalias en \n",
    "        un primer analisis. Por su parte \"clean_data\" es un booleano que nos indica si queremos realizar el analisis con \n",
    "        el dataframe ya limpio o no (para esto ya se deber√≠an haber limpiado los datos).  \n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        l : list\n",
    "        Liste de columnas a analizar, en caso de ser None, se analizan todas\n",
    "\n",
    "        threshold : float\n",
    "        valor utilizado para determinar outliers en funcion de los quantiles\n",
    "\n",
    "        clean_data : bool\n",
    "        booleano que indica si se utiliza la data limpia o no.\n",
    "    \n",
    "        Return\n",
    "        ------\n",
    "        descripcion: None\n",
    "        Esta funci√≥n solo crea un archivo de texto en donde se plasma el an√°lisis\n",
    "        \"\"\"\n",
    "        summary_file = os.path.join(self.output_folder, \"summary.txt\")\n",
    "        with open(summary_file, \"a\") as f:\n",
    "            df = self.df.copy()\n",
    "            if clean_data:\n",
    "                df = self.data_clean.copy()\n",
    "            if l == None:\n",
    "                l = df.columns.to_list()\n",
    "            for variable in l:\n",
    "                f.write(f\"Variable: {variable}\\n\")\n",
    "                f.write(\"Tipo: \" + str(df[variable].dtype) + \"\\n\")\n",
    "                filtered_data = df[variable]\n",
    "                \n",
    "                unicos = filtered_data.nunique()\n",
    "                totales = len(filtered_data)\n",
    "                nulos = filtered_data.isnull().sum()\n",
    "                \n",
    "                f.write(f\"Total de valores √∫nicos: {unicos}\\n\")\n",
    "                f.write(f\"Porcentaje de valores √∫nicos: {(unicos / totales) * 100:.2f}%\\n\")\n",
    "                f.write(f\"N√∫mero de valores nulos: {nulos}\\n\")\n",
    "                f.write(f\"Porcentaje de valores nulos: {(nulos / totales) * 100:.2f}%\\n\")\n",
    "                \n",
    "                if is_numeric_dtype(filtered_data):\n",
    "                    f.write(\"Resumen variable num√©rica:\\n\")\n",
    "                    f.write(f\"M√≠nimo: {filtered_data.min()}\\n\")\n",
    "                    f.write(f\"M√°ximo: {filtered_data.max()}\\n\")\n",
    "                    f.write(f\"Promedio: {filtered_data.mean()}\\n\")\n",
    "                    f.write(f\"Percentil 25: {filtered_data.quantile(0.25)}\\n\")\n",
    "                    f.write(f\"Percentil 50 (Median): {filtered_data.median()}\\n\")\n",
    "                    f.write(f\"Percentil 75: {filtered_data.quantile(0.75)}\\n\")\n",
    "                    f.write(f\"Percentil 100: {filtered_data.max()}\\n\")\n",
    "                    f.write(f\"Cantidad de 0's: {(filtered_data == 0).sum()}\\n\")\n",
    "                    f.write(f\"Porcentaje de 0's: {((filtered_data == 0).sum() / totales) * 100:.2f}%\\n\")\n",
    "                    f.write(f\"Cantidad de negativos: {(filtered_data < 0).sum()}\\n\")\n",
    "                    f.write(f\"Porcentaje de negativos: {((filtered_data < 0).sum() / totales) * 100:.2f}%\\n\")\n",
    "                    \n",
    "                    if threshold!=None:\n",
    "                        outliers = (filtered_data < filtered_data.quantile(threshold)) | (filtered_data > filtered_data.quantile(1 - threshold))\n",
    "                        f.write(f\"Outliers ({threshold * 100}% threshold): {outliers.sum()}\\n\")\n",
    "                        if outliers.sum() > 0:\n",
    "                            f.write(\"Alert: Outliers detected!\\n\")\n",
    "    \n",
    "                f.write(\"\\n\")      \n",
    "    # \"\"\" El m√©todo `plot_vars`, el cual debe graficar la distribuci√≥n e interraciones de las variables del Dataset. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    # - Crear la carpeta `EDA_fecha/plots`\n",
    "    # - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    # - Para las variables num√©ricas:\n",
    "    #     - Genere un gr√°fico de distribuci√≥n de densidad\n",
    "    #    - Grafique la correlaci√≥n entre las variables\n",
    "    # - Para las variables categ√≥ricas:\n",
    "    #     - Genere un histograma de las top N categor√≠as (N debe ser un par√°metro ajustable)\n",
    "    #     - Grafique el coeficiente V de Cramer entre las variables\n",
    "    # - Guardar cada gr√°fico generado en la carpeta `EDA_fecha/plots` en formato `.pdf` y bajo el naming `variable.pdf`, donde `variable` es el nombre de la variable de inter√©s \n",
    "    # \"\"\" \n",
    "    def plot_vars(self, variables = None, N=10, clean_data = False):\n",
    "        \"\"\" La funci√≥n de la clase Profiler que se encarga de plotear las variables del dataframe intentando plasmar \n",
    "        que relaciones existen entre las variables y mostrar una breve descripcion de estas. El par√°metro variables es \n",
    "        una lista que indica que variables ser√°n ploteadas, \"N\" representa el top \"N\" que se mostrar√° en las variables \n",
    "        categ√≥ricas y \"clean_data\" es un booleano que nos indica si queremos realizar el analisis con \n",
    "        el dataframe ya limpio o no (para esto ya se deber√≠an haber limpiado los datos). Es importante que al plotear \n",
    "        se analizan relaciones entre variables numericas y categ√≥ricas.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        variables : list\n",
    "        Lista de columnas a plotear, en caso de ser None, se plotean todas\n",
    "\n",
    "        N : int\n",
    "        valor utilizado mostrar el top N de datos categ√≥ricos\n",
    "\n",
    "        clean_data : bool\n",
    "        booleano que indica si se utiliza la data limpia o no (data que debe ser previamente obtenida).\n",
    "    \n",
    "        Return\n",
    "        ------\n",
    "        descripcion: None\n",
    "        Esta funci√≥n solo crea archivos formato pdf con las imagenes de la data procesada\n",
    "        \"\"\"\n",
    "        # vamos a la carpeta deseada\n",
    "        plots_folder = os.path.join(self.output_folder, \"plots\")\n",
    "        # En caso de no existir la carpeta se crea\n",
    "        if not os.path.exists(plots_folder):\n",
    "            os.mkdir(plots_folder)\n",
    "        \n",
    "        data = self.df.copy()\n",
    "        if clean_data:\n",
    "            data = self.data_clean.copy()\n",
    "        if variables == None:\n",
    "            variables = data.columns.to_list()\n",
    "        num = []\n",
    "        cat = []\n",
    "        for variable in variables:\n",
    "            if is_numeric_dtype(data[variable]) or is_datetime_or_timedelta_dtype(data[variable]):\n",
    "                num.append(variable)\n",
    "            elif is_object_dtype(data[variable]):\n",
    "                cat.append(variable)\n",
    "\n",
    "\n",
    "        # Filtramos la data para el caso de variables numericas\n",
    "        filtered_data = data[num]\n",
    "        for variable in num:\n",
    "            # Gr√°fico de distribuci√≥n de densidad\n",
    "            plt.figure()\n",
    "            sns.histplot(filtered_data[variable], kde=True)\n",
    "            plt.title(f\"Distribution of {variable}\")\n",
    "            plt.xlabel(variable)\n",
    "            plt.ylabel(\"Densidad\")\n",
    "            plt.savefig(os.path.join(plots_folder, f\"{variable}.pdf\"))\n",
    "            plt.close()\n",
    "        if len(num)>1:\n",
    "            # Gr√°fico de correlaci√≥n variables num√©ricas \n",
    "            plt.figure()\n",
    "            corr_matrix = filtered_data.corr()\n",
    "            sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\n",
    "            plt.title(f\"Matriz de correlacion {'_'.join(num)}\")\n",
    "            plt.savefig(os.path.join(plots_folder, f\"{'_'.join(num)}_matriz_correlacion.pdf\"))\n",
    "            plt.close()\n",
    "\n",
    "        # Luego de trabajar con variables numericas trabajamos con variables \n",
    "        # categoricas\n",
    "        for variable in cat:\n",
    "            filtered_data = data[variable]\n",
    "            #Obtenemos el top N\n",
    "            top_categories = filtered_data.value_counts().nlargest(N).index\n",
    "            filtered_data = filtered_data.where(filtered_data.isin(top_categories), \"Other\")\n",
    "\n",
    "            plt.figure()\n",
    "            sns.histplot(filtered_data, discrete=True)\n",
    "            plt.title(f\"Histograma de {variable}\")\n",
    "            plt.xlabel(variable)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.ylabel(\"Frecuencia\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(plots_folder, f\"{variable}.pdf\"),dpi=199)\n",
    "            plt.close()\n",
    "        if len(cat)>1:\n",
    "        \n",
    "            corrM = np.zeros((len(cat),len(cat)))\n",
    "            # there's probably a nice pandas way to do this\n",
    "            for col1, col2 in itertools.combinations(cat, 2):\n",
    "                idx1, idx2 = cat.index(col1), cat.index(col2)\n",
    "                corrM[idx1, idx2] = cramers_corrected_stat(pd.crosstab(data[col1], data[col2]))\n",
    "                corrM[idx2, idx1] = corrM[idx1, idx2]\n",
    "            corr = pd.DataFrame(corrM, index=cat, columns=cat)\n",
    "            fig, ax = plt.subplots(figsize=(7, 6))\n",
    "            ax = sns.heatmap(corr, annot=True, ax=ax);            \n",
    "            plt.title(\"Matriz Coeficientes V de Cramer\")\n",
    "            plt.savefig(os.path.join(plots_folder, f\"{'_'.join(cat)}_coeficiente_Cramer_V.pdf\"))\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "    \n",
    "    # \"\"\"El m√©todo `clean_data`, el cual debe limpiar los datos para que luego puedan ser procesados. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    # - Crear la carpeta `EDA_fecha/clean_data`\n",
    "    # - Implementar una funcionalidad para filtrar y aplicar este m√©todo a una o m√°s variables de inter√©s.\n",
    "    # - Drop de valores duplicados\n",
    "    # - Implementar como m√≠nimo 2 t√©cnicas para tratar los valores nulos, como:\n",
    "    #     - Drop de valores nulos\n",
    "    #     - Imputar valores nulos con alguna t√©cnica de imputaci√≥n\n",
    "    #     - Funcionalidad para escoger entre una t√©cnica y la otra.\n",
    "    # - Una de las columnas del dataframe presenta datos *no at√≥micos*. Separe dicha columna en las columnas que la compongan.\n",
    "    #     - Hint: ¬øQu√© caracteres permiten separar una columna de otra?\n",
    "    #     - Para las pruebas con el dataset nuevo, puede esperar que exista al menos una columna con este tipo de problema. Asuma que los separadores ser√°n los mismos, aunque el n√∫mero de columnas a separar puede ser distinto.\n",
    "    # - Deber√≠an usar `FunctionTransformer`.\n",
    "    # - Guardar los datos procesados en formato `.csv` en el path `EDA_fecha/clean_data/data.csv`\n",
    "    # \"\"\"\n",
    "    def clean_data(self,columns = None, columns_to_split=None,type_data_split = None, duplicados=False, drop_na=False, impute_na=None):\n",
    "        \"\"\" Funci√≥n de la clase Profiler que se encarga de Limpiar la data que se tiene y hacer un mini-preprocesamiento \n",
    "        al separar columnas con datos no at√≥micos (ej: edad-altura-peso, una columna que contenga estos datos separados por \n",
    "        alg√∫n separador), esta funci√≥n recibe \"columns\" que son las columnas que se desean limpiar y por tanto las que \n",
    "        tendr√° el dataset de salida, \"columns_to_split\" conteine el nombre de las columnas que contienen datos no at√≥micos \n",
    "        para ser separados, type_data_split es un diccionario que tendr√° como llaves las columnas a diidir y como valor \n",
    "        el tipo de dato al cual ser√°n seteadas las nuevas columnas (todas se setear√°n a un solo tipo de dato), duplicados\n",
    "        es un booleano que indica si se desean eliminar duplicados o no, drop_na es un booleano que indica si se desean \n",
    "        eliminar valores faltantes o tipo nan, impute_na por su parte es una funci√≥n que se utilizar√° para imputar en \n",
    "        los casos en que el valor sea na, esta funci√≥n deber√≠a estar en el formato adecuado para incluirse en un pipeline y\n",
    "        aplicarse al dataframe.\n",
    "\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        columns : list\n",
    "        Lista de columnas a usar, en caso de ser None, se utilizar√°n todas\n",
    "\n",
    "        columns_to_split : list\n",
    "        Lista con el nombre de las columnas con datos no at√≥micos que ser√°n saparadas \n",
    "\n",
    "        type_data_split : dicc\n",
    "        Diccionario que contiene como llave la columna a dividir y como valor el typo de dato al cual ser√°n seteadas\n",
    "        estas variables (todas seteadas con el mismo tipo)\n",
    "\n",
    "        duplicados: bool\n",
    "        Booleano que indica si ser√°n o no eliminados los valores duplicados\n",
    "\n",
    "        drop_na: bool\n",
    "        Booleano que indica si ser√°n o no eliminados los valores na, nan o faltantes\n",
    "\n",
    "        imput_na: callable\n",
    "        es una funci√≥n que ser√° utilizada para imputar valores en el dataframe (debe estar lista para ser utlizada en el pipeline)\n",
    "    \n",
    "        Return\n",
    "        ------\n",
    "        descripcion: pd.Dataframe, Pipeline\n",
    "        Esta funci√≥n guarda la data limpia y el Pipeline utilizado\n",
    "        \"\"\"\n",
    "        clean_data_folder = os.path.join(self.output_folder, \"clean_data\")\n",
    "        # En caso de no existir la carpeta, esta se crea \n",
    "        if not os.path.exists(clean_data_folder):\n",
    "            os.mkdir(clean_data_folder)\n",
    "\n",
    "        clean_data = self.df.copy()\n",
    "        l_pipe = []\n",
    "        if columns != None:\n",
    "            \n",
    "            column_selector = FunctionTransformer(\n",
    "                func=lambda X: X[columns],\n",
    "                validate=False\n",
    "            )\n",
    "            l_pipe.append(('select_columns', column_selector))\n",
    "            #clean_data = clean_data[columns]\n",
    "            \n",
    "        if columns_to_split != None:\n",
    "            \n",
    "            column_spliter_tf = FunctionTransformer(\n",
    "                func=lambda X: column_spliter(X, columns_to_split, type_data_split),\n",
    "                validate=False\n",
    "            )\n",
    "            l_pipe.append(('columns_spliter', column_spliter_tf))\n",
    "\n",
    "        if impute_na != None:\n",
    "            l_pipe.append(('imputer_na', impute_na))\n",
    "            #imputer = FunctionTransformer(impute_na)\n",
    "            #clean_data = imputer.fit_transform(clean_data)\n",
    "            \n",
    "        if duplicados:\n",
    "            duplicates_droper = FunctionTransformer(\n",
    "                func=lambda X: X.drop_duplicates(),\n",
    "                validate=False  # Puedes ajustar esto seg√∫n tus necesidades\n",
    "            )\n",
    "            l_pipe.append(('drop_duplicates', duplicates_droper))\n",
    "            #clean_data = clean_data.drop_duplicates()\n",
    "\n",
    "        if drop_na:\n",
    "            na_droper = FunctionTransformer(\n",
    "                func=lambda X: X.dropna(),\n",
    "                validate=False  # Puedes ajustar esto seg√∫n tus necesidades\n",
    "            )\n",
    "            l_pipe.append(('drop_na', na_droper))\n",
    "            #clean_data = clean_data.dropna()\n",
    "        pipe_line = None\n",
    "        if len(l_pipe)>0:\n",
    "            pipe_line = Pipeline(l_pipe)\n",
    "            #clean_data = clean_data.reset_index(drop = True)\n",
    "            clean_data = pipe_line.fit_transform(clean_data)\n",
    "            self.clean_pipeline = pipe_line\n",
    "        clean_data.to_csv(os.path.join(clean_data_folder, \"data.csv\"), index=False)\n",
    "        self.data_clean = clean_data\n",
    "        return clean_data, pipe_line\n",
    "\n",
    "\n",
    "# 5. El m√©todo `scale`, el cual debe preparar adecuadamente los datos para luego ser consumidos por alg√∫n tipo de algoritmo. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "#     - Crear la carpeta `EDA_fecha`\n",
    "#     - Procesar de forma adecuada los datos num√©ricos y categ√≥ricos:\n",
    "#         - Su m√©todo debe recibir las t√©cnicas de escalamiento como argumento de entrada (utilizar solo t√©cnicas compatibles con el framework de `sklearn`)\n",
    "#         - Para los atributos num√©ricos,/scale se transforme los datos con un escalador logar√≠tmico y un `MinMaxScaler`\n",
    "#         - Asuma que no existen datos ordinales en su dataset\n",
    "#     - Guardar todo este procesamiento en un `ColumnTransformer`.\n",
    "#     - Guardar los datos limpios y transformados en formato `.csv` en el path `EDA_fecha/process/scaled_features.csv`\n",
    "\n",
    "    def scale(self, categorical_transformer = None, numeric_scaler = None, l_cat = None, l_nums = None, data_clean = True):\n",
    "        \"\"\" Funci√≥n de la clase Profiler que se encarga de escalar y transbajar la data que se tiene, con el fin de procesar\n",
    "        los datos , esta funci√≥n recibe \"categorical_transformer\" que es un metodo con el cual ser√°n procesados los datos \n",
    "        categ√≥ricos \"numeric_scaler\" un metodo con el cual ser√°n procesados los datos num√©ricos, \"l_cat\" y \"l_nums\" son listas \n",
    "        que  indican las columnas que ser√°n procesadas tanto para el transformador categ√≥rico, como para los escaladores \n",
    "        num√©ricos, \"data_claen\" es un booleano que indica si utilizar la data ya limpia o no para el escalado. \n",
    "\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        categorical_transformer : callable\n",
    "        funci√≥n que se utilizar√° para transformar las columnas categoricas\n",
    "\n",
    "        numeric_scaler : callable\n",
    "        funci√≥n que se utilizar√° para transformar las columnas num√©ricas\n",
    "\n",
    "        l_cat : list\n",
    "        Lista con columnas categ√≥ricas a transformar\n",
    "\n",
    "        l_nums: list\n",
    "        Lista con columnas num√©ricas a transformar\n",
    "\n",
    "        data_clean: bool\n",
    "        Booleano que indica si se usar√° o no la data limpia para el escalado\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        descripcion: pd.Dataframe, Pipeline\n",
    "        Esta funci√≥n guarda la data escalada y el Pipeline utilizado\n",
    "        \"\"\"\n",
    "        scale_folder = os.path.join(self.output_folder, \"process\")\n",
    "        # si no existe la carpeta es se crea \n",
    "        if not os.path.exists(scale_folder):\n",
    "            os.mkdir(scale_folder)\n",
    "        df = self.df.copy()\n",
    "        if data_clean:\n",
    "            df = self.data_clean.copy()\n",
    "        # Seleccionador las columnas que el usuario indica\n",
    "        # Identificar columnas num√©ricas y categ√≥ricas\n",
    "        numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "        categorical_columns = df.select_dtypes(exclude=[np.number]).columns\n",
    "        if l_nums != None:\n",
    "            numeric_columns = l_nums\n",
    "        if l_cat != None: \n",
    "            categorical_columns = l_cat\n",
    "        # Escalador para datos num√©ricos\n",
    "        # por default se transforma utilizando  log y minmax\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('log_scale', FunctionTransformer(np.log1p)),  # Escalado est√°ndar\n",
    "            ('minmax_scaler', MinMaxScaler())  # Escalado Min-Max\n",
    "        ])\n",
    "        if (numeric_scaler != None):\n",
    "            numeric_transformer = Pipeline(steps=[\n",
    "            ('log_scale', FunctionTransformer(np.log1p)),  # Escalado est√°ndar\n",
    "            ('minmax_scaler', MinMaxScaler()),\n",
    "            ('num_scaler', numeric_scaler)# Escalado del usuario\n",
    "        ],)\n",
    "            \n",
    "        col_transformer = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num',numeric_transformer , numeric_columns),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if categorical_transformer!= None:\n",
    "            col_transformer = ColumnTransformer(\n",
    "                transformers=[\n",
    "                ('num',numeric_transformer , numeric_columns),\n",
    "                ('cat', categorical_transformer, categorical_columns)\n",
    "                ]\n",
    "            )\n",
    "        # Aplicar el escalado a los datos\n",
    "        scaled_features = col_transformer.fit_transform(df)\n",
    "        scaled_features_df = pd.DataFrame(scaled_features) \n",
    "        self.scaled_data = scaled_features_df\n",
    "        self.scale_transform = col_transformer\n",
    "        # Guardar los datos procesados en un archivo CSV\n",
    "        #scaled_features_df.to_csv(os.path.join(scale_folder, 'scaled_features.csv'), index=False)\n",
    "        return scaled_features_df,col_transformer\n",
    "\n",
    "    # 6. El m√©todo `make_clusters`, el cual debe generar clusters de los datos usando alg√∫n algoritmo de clusterizaci√≥n. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "    #     - Crear la carpeta `EDA_fecha/clusters`\n",
    "    #     - Generar un estudio del codo donde se√±ale la cantidad de clusters optimos para el desarrollo.\n",
    "    #     - Su m√©todo debe recibir el algoritmo de clustering como argumento de entrada (utilizar solo algoritmos compatibles con el framework de `sklearn`).\n",
    "    #     - No olvide pre procesar adecuadamente los datos antes de implementar la t√©cnica de clustering. \n",
    "    #     - En este punto es espera que generen un `Pipeline` de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5. \n",
    "    #     - Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
    "    #     - Una vez generado los clusters, proyecte los datos a 2 dimensiones usando su t√©cnica de reducci√≥n de dimensionalidad favorita y grafique los resultados coloreando por cluster.\n",
    "    #     - Guardar los datos con su respectivo cluster en formato `.csv` en el path `EDA_fecha/clusters/data_clusters.csv`. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
    "\n",
    "    def make_clusters(self, clustering_algorithm, listo = False ,n_clusters_max = 10, clean_data = False):\n",
    "        \"\"\" Funci√≥n de la clase Profiler que se encarga de encontrar mediante el m√©todo del codo, el n√∫mero √≥ptimo de clusters,\n",
    "        y clasificar la data seg√∫n un algoritmo de clusterizaci√≥n recibido. Esta funci√≥n recibe \"clustering_algorithm\" que es \n",
    "        un m√©todo con el cual ser√°n procesados los datos para obtener clusters,  \"listo\" un booleano que indica si se \n",
    "        utilizar√° el n√∫mero √≥ptimo de cluster encontrado o se utilizar√° el m√©todo tal cual como viene, \"n_clusters_max\" es un\n",
    "        entero, el cual ser√° utilizado como valor m√°ximo al momento de calcular el n√∫mero √≥ptimo de clusters \"clean_data\" \n",
    "        es un booleano que indica si utilizar la data ya limpia o no para la clusterizaci√≥n. \n",
    "\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        clustering_algorithm : callable\n",
    "        funci√≥n que se utilizar√° para clusterizar los datos.\n",
    "\n",
    "        listo : bool\n",
    "        Booleano que indica si se usar√° o no la el modelo tal cual como est√°\n",
    "\n",
    "        n_clusters_max : int\n",
    "        Valor entero que indica el n√∫mero m√°ximo a probar al momento de calcular el n√∫mero √≥ptimo de clusters\n",
    "\n",
    "        clean_data: bool\n",
    "        Booleano que indica si se usar√° o no la data limpia y escalada para la clusterizaci√≥n\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        descripcion: Pipeline\n",
    "        Esta funci√≥n guarda la data clusterizada y el Pipeline utilizado\n",
    "        \"\"\"\n",
    "        \n",
    "        clusters_folder = os.path.join(self.output_folder, \"clusters\")\n",
    "        if not os.path.exists(clusters_folder):\n",
    "            os.mkdir(clusters_folder)\n",
    "\n",
    "        # Opci√≥n de utilizar datos crudos en caso de estar listos para su uso\n",
    "        \n",
    "        data = self.df\n",
    "        # Opci√≥n de utilizar datos limpiados anteriormente\n",
    "        l_pipe = []\n",
    "        if clean_data:\n",
    "            pipe_clean = self.clean_pipeline\n",
    "            pipe_scale = self.scale_transform\n",
    "            l_pipe.append(('data_cleaner', pipe_clean))\n",
    "            l_pipe.append(('data_scaler', pipe_scale))\n",
    "            #data = self.data\n",
    "        # Primero usamos Kmeans para obtener el n√∫mero de clusters optimo\n",
    "        # Con la inercia utilizamos el metodo del codo para determinar el n√∫mero √≥ptimo de clusters\n",
    "        inercia = []\n",
    "        for n_cluster in range(2,n_clusters_max):\n",
    "            l_aux = l_pipe.copy()\n",
    "            l_aux.append(('kmeans-k',KMeans(n_clusters=n_cluster, random_state = 0, n_init = 2)))\n",
    "            pipeline = Pipeline(l_aux)\n",
    "\n",
    "            # Ajustar el modelo\n",
    "            pipeline.fit(data)\n",
    "            inercia.append(pipeline.named_steps['kmeans-k'].inertia_)\n",
    "            \n",
    "        n_opt = 2\n",
    "        for i in range(2,n_clusters_max-1):\n",
    "            j = i-1\n",
    "            variacion = (inercia[j-1] - inercia[j])/inercia[0]\n",
    "            if variacion >0.1:\n",
    "                n_opt = i\n",
    "        plt.plot(np.arange(2,n_clusters_max,1), inercia, marker='o')\n",
    "        plt.title(\"Estudio del Codo - Kmeans\")\n",
    "        plt.xlabel(\"N√∫mero de Clusters\")\n",
    "        plt.ylabel(\"Inercia\")\n",
    "        plt.grid()\n",
    "        plt.savefig(os.path.join(clusters_folder, \"Inercia_kmeans.pdf\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Utilizar el n√∫mero √≥ptimo de clusters seg√∫n el estudio del codo con el algoritmo entregado por el\n",
    "        # usuario, aqu√≠ se asume que el algoritmo permite engresar el n√∫mero de cluster deseados y una semilla para \n",
    "        # la aleatoridad\n",
    "        # Esto podr√≠a no funcionar con algoritmos que calculan de forma autinoma el n√∫mero optimo de cluster \n",
    "        # por ende se sealizar√° un cambio en el c√≥digo, dando la opci√≥n al usuario de entregar un algortimo listo \n",
    "        # para utilizarse o un algoritmo que utilizar√° el n√∫mero optimo de cluster encontrado por el metododel codo.\n",
    "        # pasando de:\n",
    "        # clustering_algorithm = clustering_algorithm(n_clusters=n_opt, random_state=0)\n",
    "        # cluster_labels = clustering_algorithm.fit_predict(data)\n",
    "        # a: \n",
    "        cluster_labels = []\n",
    "        if listo:\n",
    "            #clustering_algorithm = clustering_algorithm(n_clusters=n_opt, random_state=0)\n",
    "            l_pipe.append(('cluster_algoritm', clustering_algorithm))\n",
    "            pipe_cluster = Pipeline(l_pipe)\n",
    "            cluster_labels = pipe_cluster.fit_predict(data)\n",
    "        else: \n",
    "            clustering_algorithm.set_params(n_clusters=n_opt, random_state=0)\n",
    "            l_pipe.append(('cluster_algoritm', clustering_algorithm))\n",
    "            pipe_cluster = Pipeline(l_pipe)\n",
    "            cluster_labels = pipe_cluster.fit_predict(data)\n",
    "        # Reducci√≥n de dimensionalidad a 2 dimensiones utilizando UMAP\n",
    "        if clean_data:\n",
    "            data = self.scaled_data\n",
    "        pca = PCA(n_components=2)\n",
    "        data_2d = pca.fit_transform(data)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.scatter(data_2d[:, 0], data_2d[:, 1], c=cluster_labels, cmap=\"Set3\")\n",
    "        plt.title('Scatterplot 2D')\n",
    "        plt.xlabel('Primera componente')\n",
    "        plt.ylabel('Segunda componente')    \n",
    "        plt.savefig(os.path.join(clusters_folder, \"representacion_2D.pdf\"))\n",
    "        plt.close()\n",
    "        # Crear un DataFrame con datos escalados y asignarles el cluster correspondiente\n",
    "        #data_clusters = pd.DataFrame(data, columns=data.columns)\n",
    "        data = self.df.copy()\n",
    "        if clean_data:\n",
    "            data = self.data_clean.copy()\n",
    "        data['Cluster'] = cluster_labels\n",
    "        data.to_csv(os.path.join(clusters_folder, 'data_clusters.csv'), index=False)\n",
    "        print(f\"El numero de cluster √≥ptimos es {n_opt}\")\n",
    "\n",
    "        pipe_cluster = Pipeline(l_pipe)\n",
    "        self.clustering_pipe = pipe_cluster\n",
    "        return pipe_cluster\n",
    "    # El m√©todo detect_anomalies, el cual debe detectar anomal√≠as en los datos. Como m√≠nimo, se espera que su m√©todo pueda:\n",
    "\n",
    "    # Crear la carpeta EDA_fecha/anomalies\n",
    "    # Implementar alguna t√©cnica de detecci√≥n de anomal√≠as.\n",
    "    # Al igual que el punto anterior, su m√©todo debe considerar los siguientes puntos:\n",
    "    # No olvide pre procesar de forma adecuada los datos antes de implementar la t√©cnica de detecci√≥n de anomal√≠a.\n",
    "    # En este punto es espera que generen un Pipeline de sklearn. Adem√°s, su m√©todo deber√≠a usar lo construido en los puntos 4 y 5.\n",
    "    # Su m√©todo debe ser capaz de funcionar a partir de datos crudos (se descontar√° puntaje de lo contrario).\n",
    "    # Su m√©todo debe recibir el algoritmo como argumento de entrada\n",
    "    # Una vez generado las etiquetas, proyecte los datos a 2 dimensiones y grafique los resultados coloreando por las etiquetas predichas por el detector de anomal√≠as\n",
    "    # Guardar los datos con su respectiva etiqueta en formato .csv en el path EDA_fecha/anomalies/data_anomalies.csv. Guarde tambi√©n los gr√°ficos generados en el mismo path.\n",
    "\n",
    "    def detect_anomalies(self, anomaly_detector, clean_data=False):\n",
    "        \"\"\" Funci√≥n de la clase Profiler que se encarga de encontrar anomal√≠as en la data, mediante un m√©todo que \n",
    "        recibe la funci√≥n, que es el par√°metro \"anomaly_detector\".Esta funci√≥n recibe \"clean_data\" \n",
    "        es un booleano que indica si utilizar la data ya limpia o no para la detecci√≥n de anomalias. \n",
    "\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        anomaly_detector : callable\n",
    "        funci√≥n que se utilizar√° para detectar anomal√≠as en los datos.\n",
    "\n",
    "        clean_data: bool\n",
    "        Booleano que indica si se usar√° o no la data limpia y escalada.\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        descripcion: Pipeline\n",
    "        Esta funci√≥n guarda la data etiquetada y el Pipeline utilizado\n",
    "        \"\"\"\n",
    "        anomalies_folder = os.path.join(self.output_folder, \"anomalies\")\n",
    "        if not os.path.exists(anomalies_folder):\n",
    "            os.mkdir(anomalies_folder)\n",
    "\n",
    "         # Opci√≥n de utilizar datos crudos en caso de estar listos para su uso\n",
    "        data = self.df\n",
    "        # Opci√≥n de utilizar datos limpiados anteriormente\n",
    "        l_pipe = []\n",
    "        if clean_data:\n",
    "            pipe_clean = self.clean_pipeline\n",
    "            pipe_scale = self.scale_transform\n",
    "            l_pipe.append(('data_cleaner', pipe_clean))\n",
    "            l_pipe.append(('data_scaler', pipe_scale))\n",
    "            data  = self.scaled_data\n",
    "        # Detector de anomal√≠as\n",
    "        # Utilizamos el metodo de detecci√≥n entregado por el usuario, asumiendo que viene listo para su uso.\n",
    "        \n",
    "        anomaly_labels = anomaly_detector.fit_predict(data)\n",
    "\n",
    "        # PCA para reducir dimensionalidad\n",
    "        pca = PCA(n_components=2)\n",
    "        data_2d = pca.fit_transform(data)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.scatter(data_2d[anomaly_labels == 1, 0], data_2d[anomaly_labels == 1, 1], c='b', label='Normales')\n",
    "        plt.scatter(data_2d[anomaly_labels == -1, 0], data_2d[anomaly_labels == -1, 1], c='r', label='Anomal√≠as')\n",
    "        plt.title('Detector de anomalias representaci√≥n 2D')\n",
    "        plt.xlabel('Primera Componente')\n",
    "        plt.ylabel('Segunda Componente')\n",
    "        plt.legend(loc='best')\n",
    "        plt.savefig(os.path.join(anomalies_folder, \"anomalias_representacion_2D.pdf\"))\n",
    "        plt.close()\n",
    "        # Dataframe con anomal√≠as \n",
    "        #data_anomalia = pd.DataFrame(data, columns=data.columns)\n",
    "        data = self.df.copy()\n",
    "        if clean_data:\n",
    "            data = self.data_clean.copy()\n",
    "        data['Anomaly_label'] = anomaly_labels\n",
    "        data.to_csv(os.path.join(anomalies_folder, 'data_anomalies.csv'), index=False)\n",
    "        l_pipe.append(('anomaly_detector', anomaly_detector))\n",
    "        anomaly_pipe = Pipeline(l_pipe)\n",
    "        return anomaly_pipe\n",
    "    #8. El m√©todo `profile`, el cual debe ejecutar todos los m√©todos anteriores.\n",
    "    # Para correr este metodo es necesario haber corrido los m√©todos anteriores, ya que por default utiliza la data ya limpia.\n",
    "    def profile(self,columns = None,  clustering_algorithm = KMeans(n_init = 10), anomaly_detector = OneClassSVM(gamma='auto'), clean_data=True):\n",
    "        \"\"\" Funci√≥n de la clase Profiler que se encarga de correr todos las funciones, para est la data ya deb√≠o ser limpiada\n",
    "        y escalada. Esta funci√≥n recibe \"columns\", que ser√°n als columnas utilizadas para ejecutar summarize y plot_vars, adem√°s \n",
    "        recibe \"clustering_algorithm\" que ser√° el algoritmo encargado de clusterizar la data, \"anomaly_detector\" es el m√©todo \n",
    "        para detectar anomal√≠as, \"clean_data\" es un booleano que indica si utilizar la data ya limpia o no para la \n",
    "        ejecuci√≥n de todos los m√©todos. \n",
    "\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        columns: list\n",
    "        Contiene las columnas que se utilizar√°n en los m√©todos summarize y plot_vars\n",
    "        \n",
    "        clustering_algorithm : callable\n",
    "        funci√≥n que se utilizar√° para clusterizar los datos.\n",
    "        \n",
    "        anomaly_detector : callable\n",
    "        funci√≥n que se utilizar√° para detectar anomal√≠as en los datos.\n",
    "\n",
    "        clean_data: bool\n",
    "        Booleano que indica si se usar√° o no la data limpia y escalada.\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        descripcion: None\n",
    "        Esta funci√≥n guarda la data y los pipelines utilizados \n",
    "        \"\"\"\n",
    "        self.summarize(l = columns, clean_data = clean_data)\n",
    "        self.plot_vars(variables = columns, clean_data = clean_data)\n",
    "        # self.clean_data()\n",
    "        # self.scale()\n",
    "        self.make_clusters(clustering_algorithm, clean_data = clean_data)\n",
    "        #anomaly_detector = OneClassSVM(gamma='auto')\n",
    "        self.detect_anomalies(anomaly_detector, clean_data = clean_data)\n",
    "\n",
    "    #9. Crear el m√©todo `clearGarbage` para eliminar las carpetas/archivos creados/as por la clase `Profiler`.\n",
    "\n",
    "    def clearGarbage(self):\n",
    "        \"\"\"Esta funci√≥n seencarga de limpiar las carperas generadas por un elemento de esta clase\n",
    "        elliminando todos los registros para dicho elemento.\"\"\"\n",
    "        # Eliminar carpetas y archivos creados por la clase Profiler\n",
    "        carpeta = self.output_folder\n",
    "        if os.path.exists(self.output_folder):\n",
    "            try:\n",
    "                # Eliminar recursivamente la carpeta de salida y su contenido\n",
    "                shutil.rmtree(carpeta)\n",
    "                print(f\"Se elimin√≥ la carpeta {carpeta} y su contenido.\")\n",
    "            except Exception as e:\n",
    "                print(f\"No se pudo eliminar la carpeta {carpeta}: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Caracterizar datos de Olimpiadas (2.0 puntos)\n",
    "\n",
    "A partir de la clase que hemos desarrollado previamente, procederemos a realizar un an√°lisis exhaustivo de los datos proporcionados en el enunciado. Este an√°lisis se presentar√° en forma de un informe contenido en el mismo Jupyter Notebook y abordar√° los siguientes puntos:\n",
    "\n",
    "1. Introducci√≥n\n",
    "    - Se proporcionar√° una breve descripci√≥n del problema que estamos abordando y se explicar√° la metodolog√≠a que se seguir√°.\n",
    "\n",
    "Elaborar una breve introducci√≥n con todo lo necesario para entender qu√© realizar√°n durante su proyecto. La idea es que describan de manera formal el proyecto con sus propias palabras y logren describir algunos aspectos b√°sicos tanto del dataset como del an√°lisis a realizar sobre los datos.\n",
    "\n",
    "Por lo anterior, en esta secci√≥n ustedes deber√°n ser capaces de:\n",
    "\n",
    "- Describir la tarea asociada al dataset.\n",
    "- Describir brevemente los datos de entrada que les provee el problema.\n",
    "- Plantear hip√≥tesis de c√≥mo podr√≠an abordar el problema.\n",
    "\n",
    "2. An√°lisis del EDA (An√°lisis Exploratorio de Datos)\n",
    "    - Se discutir√°n las observaciones y conclusiones obtenidas acerca de los datos proporcionados. A lo largo de su respuesta, debe responder preguntas como:\n",
    "        - ¬øComo se comportan las variables num√©ricas? ¬øy las categ√≥ricas?\n",
    "        - ¬øExisten valores nulos en el dataset? ¬øEn qu√© columnas? ¬øCuantos?\n",
    "        - ¬øCu√°les son las categor√≠as y frecuencias de las variables categ√≥ricas?\n",
    "        - ¬øExisten datos duplicados en el conjunto?\n",
    "        - ¬øExisten relaciones o patrones visuales entre las variables?\n",
    "        - ¬øExisten anomal√≠as notables o preocupantes en los datos?\n",
    "3. Creaci√≥n de Clusters y Anomal√≠as\n",
    "    - Se justificar√° la elecci√≥n de los algoritmos a utilizar y sus hiperpar√°metros. En el caso de clustering, justifique adem√°s el n√∫mero de clusters.\n",
    "    \n",
    "4. An√°lisis de Resultados\n",
    "    - Se examinar√°n los resultados obtenidos a partir de los cl√∫sters y anomal√≠as generadas. ¬øSe logra una separaci√≥n efectiva de los datos? Entregue una interpretaci√≥n de lo que representa cada cl√∫ster y anomal√≠a.\n",
    "5. Conclusi√≥n\n",
    "    - Se resumir√°n las principales conclusiones del an√°lisis y se destacar√°n las implicaciones pr√°cticas de los resultados obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Introducci√≥n\n",
    "\n",
    "En el contexto de los Juegos Santiago 2023, se espera desarrollar un an√°lisis detallado de los datos generados por el evento. El objetivo principal es extraer informaci√≥n y patrones significativos de los datos. Este an√°lisis busca proporcionar una visi√≥n clara de los aspectos deportivos y socioecon√≥micos relevantes durante los juegos.\n",
    "\n",
    "La metodolog√≠a para abordar este desaf√≠o incluye la creaci√≥n de una clase Profiler en Python, con funciones espec√≠ficas para cargar y explorar datos, detectar anomal√≠as y generar clusters. Se utilizar√°n herramientas como el EDA para comprender las caracter√≠sticas del conjunto de datos, seguido de t√©cnicas avanzadas para la agrupaci√≥n de datos y la identificaci√≥n de anomal√≠as. Los datos a trabajar incluyen variables como edad, peso, altura, deporte, sexo, pa√≠s (NOC), temporada, y medallas. El an√°lisis se enfocar√° en descubrir patrones, correlaciones y tendencias significativas en estos datos.\n",
    "\n",
    "Se espera que el an√°lisis revele patrones interesantes en relaci√≥n con el rendimiento de los atletas, la distribuci√≥n de medallas, y posibles correlaciones entre las variables f√≠sicas de los atletas y su √©xito en los juegos. Adem√°s, esperamos identificar grupos distintos de atletas y eventos a trav√©s del an√°lisis de clustering, as√≠ como detectar cualquier anomal√≠a que pueda indicar datos inusuales.\n",
    "\n",
    "2. An√°lisis del EDA\n",
    "\n",
    "a. Caracterizaci√≥n del Dataset\n",
    "\n",
    "    Valores nulos:\n",
    "        La √∫nica columna con valores nulos es 'Medal', que tiene 231,333 valores nulos. Esto significa que una proporci√≥n importante de los datos no tiene informaci√≥n sobre medallas ganadas, indicando que muchos atletas no ganaron medallas en sus eventos. As√≠, es importante considerarlo al realizar an√°lisis relacionados con el rendimiento, y en todo lo que viene m√°s adelante.\n",
    "\n",
    "    Categor√≠as y frecuencias de las variables categ√≥ricas:\n",
    "        En primer lugar, tenemos variables como 'Sex' que tienen solo dos posibles opciones, Masculino (M) o Femenino (M) con 196.584 y 74.522 casos respectivamente.Por otro lado, 'Team', 'NOC', 'Games', 'City', 'Sport', 'Event' son variables con una amplia variedad de categor√≠as. Por ejemplo, 'Team' tiene 1,184 categor√≠as √∫nicas, 'NOC' tiene 230, y 'Sport' tiene 66. Las frecuencias var√≠an considerablemente, con algunos equipos, naciones o deportes apareciendo con mucha m√°s frecuencia que otros.\n",
    "        Finalmente, tenemos 'Medal' con tres posibles opciones, siendo estas: Oro - 13,372, Bronce - 13,295, Plata - 13,116.\n",
    "\n",
    "    Datos duplicados:\n",
    "        Hay 668 filas duplicadas en el conjunto de datos, lo que sugiere la necesidad de una limpieza adicional del conjunto de datos para asegurar la precisi√≥n en el an√°lisis futuro.\n",
    "\n",
    "    Anomal√≠as notables o preocupantes en los datos:\n",
    "        Una anomal√≠a notable es la gran cantidad de valores nulos en la columna 'Medal'. Por otro lado, la columna 'age-height-weight' presenta un desaf√≠o debido a su formato inconsistente. Esto sugiere que se requiere una limpieza y normalizaci√≥n de datos antes de que se pueda realizar un an√°lisis significativo en estas √°reas.\n",
    "\n",
    "b. Comportamiento de las Variables Num√©ricas\n",
    "\n",
    "    Las variables num√©ricas como peso, altura y edad muestran distribuciones que se aproximan a la normal, con el peso y la altura mostrando una correlaci√≥n positiva fuerte (0.8), lo cual es t√≠pico en poblaciones humanas. La edad presenta una distribuci√≥n m√°s concentrada, lo que lleva a pensar que hay una edad promedio m√°s com√∫n entre los atletas, con menos atletas en los extremos j√≥venes y mayores de la distribuci√≥n.\n",
    "\n",
    "c. Comportamiento de las Variables Categ√≥ricas\n",
    "\n",
    "    En cuanto a las variables categ√≥ricas, estas corresponden a deporte, sexo, temporada, NOC (Comit√©s Ol√≠mpicos Nacionales). De estas, es posible concluir que hay una clara desigualdad en la distribuci√≥n de g√©nero con una mayor representaci√≥n masculina. Adem√°s, la temporada de verano es mucho m√°s representada que la de invierno.\n",
    "    \n",
    "\n",
    "3. Creaci√≥n de Clusters y Anomal√≠as\n",
    "\n",
    "En este caso, el algoritmo de Clustering a utilizar corresponde a K-Means. Este algoritmo es ampliamente utilizado debido a su simplicidad y eficacia, los clusters generados por este algoritmo son f√°ciles de interpretar, lo que es √∫til para la comunicaci√≥n de resultados y el an√°lisis posterior. Finalmente, y respecto al √∫ltimo punto de la justificaci√≥n de si utilizaci√≥n, K-Means puede manejar grandes conjuntos de datos de manera eficiente, lo que es importante cuando se trabaja con muchos datos.\n",
    "\n",
    "En cuanto a los hiperpar√°metros utilizados, se tiene que el m√©todo del codo fue utilizado para determinar el n√∫mero adecuado de clusters. Este permite seleccionar el valor de k donde la disminuci√≥n de la inercia parece disminuir, indicando un punto de \"rendimientos decrecientes\". De esta forma, el punto de codo en el gr√°fico proporcionado sugiere que despu√©s de 4 a 6 clusters, los beneficios de aumentar el n√∫mero de clusters se reducen, lo que justifica seleccionar un k dentro de ese rango. Adem√°s, esto se ve validado por la existencia de categor√≠as que verifican el rendimiento de lso atletas, que presentan esa exacta cantidad de posibles valores.\n",
    "\n",
    "Por otro lado, en cuanto a las anomal√≠as, se lleva a cabo la identificaci√≥n de Outliers. El objetivo principal, es identificar observaciones que no siguen el patr√≥n de la mayor√≠a de los datos, lo que es √∫til para limpiar el conjunto de datos antes de realizar un an√°lisis m√°s profundo o para identificar eventos o elementos raros que merecen una investigaci√≥n adicional.\n",
    "\n",
    "4. An√°lisis de Resultados\n",
    "\n",
    "\n",
    "Como resultados, fue posible obtener visualizaciones de las anomal√≠as y los clusters generados. En primer lugar, se lleva a cabo un scatterplot que muestra una clara distinci√≥n entre los datos normales y las anomal√≠as, y este sugiere que el algoritmo de detecci√≥n de anomal√≠as ha sido efectivo en separar los outliers del conjunto de datos principal. Los datos normales parecen centrarse en torno a un punto central en el espacio de la primera y segunda componente, lo que es de esperar, ya que la mayor√≠a de las observaciones se agrupan en un √°rea com√∫n del espacio de caracter√≠sticas. Por otro lado, las anomal√≠as tienden a ubicarse en la periferia del scatterplot, lo cual es consistente con la intuici√≥n de que las observaciones que se desv√≠an significativamente de la norma se consideran outliers.\n",
    "\n",
    "En segundo lugar, se tiene el gr√°fico de codo, que muestra la inercia para diferentes n√∫meros de clusters. La inercia disminuye a medida que aumenta el n√∫mero de clusters, y el \"punto de codo\" es el punto donde la disminuci√≥n de la inercia se ralentiza significativamente. En el gr√°fico, parece que el codo est√° en los 3 clusters, por lo que se decide trabajar con 3. \n",
    "\n",
    "Luego, a partir de esto, fue generado el gr√°fico que muestra los clusters, con los datos transformados a dos componentes principales. En este, se puede observar que los datos se agrupan en tres distintas regiones en el espacio de las dos primeras componentes principales. Del analisis posterior realizado a los datos clasificados tenemos los siguientes resultados:\n",
    "\n",
    "| Cluster | Age  |     |     |     |Heigh |     |     |     |Weigh |     |     |     | F | M |\n",
    "| ---     |  --- | --- | --- | --- | ---  | --- | --- | --- | ---  | --- | --- | --- | - | - |\n",
    "|         | Mean | Std | M√≠n | M√°x | Mean | Std | M√≠n | M√°x | Mean | Std | M√≠n | M√°x |   |   |\n",
    "| 0\t      |24.41 | 3.5 |41.0 |15.0 |187.3 |7.1  |223.0|167.0|86.2  |12.14|182.0|59.0 |1314|10775|\n",
    "| 1\t      |30.51 | 4.76|66.0 |24.0 |175.5 |6.2  |198.0|147.0|71.36 |9.23 |118.0|41.0 |3055|5396 |\n",
    "| 2\t      |22.25 | 3.26|34.0 |13.0 |167.3 |7.2  |185.0|136.0|60.25 |8.00 |95.0 |28.0 |5981|3660 |\n",
    "\n",
    "Al observar detenidamente los cluster es posible ver que en el cluster \"0\" est√°n las personas de mayor altura y peso principalmente, fijandonos en la edad son en su mayor√≠a jovenes. Por su parte en el cluster \"1\" el tanto el promedio de edad como el rango son significatibamente mayores, en cuanto a peso y altura vendr√≠a siendo el cluster con valores intermedios. Por otro lado el cluster \"2\" tiene un promedio de edad similar al \"0\" y la principal diferencia se observa en peso y altura, ya que este √∫ltimo cluster tiene valores m√°s peque√±os que los anteriores.\n",
    "\n",
    "Es importante mencionar que esta clusterizaci√≥n se realiz√≥ utilizando unicamente data de \"Age\", \"Heigh\" y \"Weigh\", ya que K-means suele funcionar mucho mejor con data num√©rica, con esto en cuante al ver el numero de personas por sexo en cada cluster vemos el el primer cluster es un cluster principalmente compuesto por Hombres y el √∫ltimo es un cluster mayoritariamente de mujeres. Esto nos muestra que a pesar de no estar utilizando directamente la variable sexo en nuestro an√°lisis hay otras variables relacionadas con esta que podr√≠an permitir clasificar la data de forma similar a la separaci√≥n por sexo.\n",
    "\n",
    "5. Conclusi√≥n\n",
    "   El trabajo realizado da cuenta de la importancia que tiene realizar un buen EDA y m√°s a√∫n de realizarlo de forma estructurada para as√≠ facilitar la comprensi√≥n de los futuros lectores, de manera que puedan replicar los resultados y modificar los pasos que estimen convenientes.\n",
    "   Al trabajar con los datos y probar las funciones se observ√≥ lo dificil que es hacer metodos de an√°lisis gen√©ricos, ya que por ejemplo al hacer OneHotEndoding y tener  muchas categ√≥rias los resultados dejaban de ser representables en arrays, o al tener muchos datos los algoritmos de clustering, reducci√≥n de dimensionalidad y detecci√≥n de anomalias se ca√≠an, generando problemas en el desarrollo del proyecto. Por esto se opto por utilizar algorimos conocidos, r√°pidos y sencillos, en la mayor√≠a de los casos con sus argumentos por default, para dejar esta herramienta lo m√°s neutra y generalizable posible.\n",
    "   De todas maneras creemos que en el futuro al momento de realizar un EDA, si bien los pasos pueden ser similares, ser√° indispensable crear distitnas clases o pipelines para cada set de datos ya que, hay una gran cantidad de herramientas disponibles y cada una de ellas tiene caracter√≠sticas que podr√≠an ser o no √∫tiles en cada caso, para esto es necesario tener en cuenta, tanto el tipo de datos con los que se est√° trabajando y los objetivos que se buscan alcanzar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Team</th>\n",
       "      <th>NOC</th>\n",
       "      <th>Games</th>\n",
       "      <th>Year</th>\n",
       "      <th>Season</th>\n",
       "      <th>City</th>\n",
       "      <th>Sport</th>\n",
       "      <th>Event</th>\n",
       "      <th>Medal</th>\n",
       "      <th>age-height-weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A Dijiang</td>\n",
       "      <td>M</td>\n",
       "      <td>China</td>\n",
       "      <td>CHN</td>\n",
       "      <td>1992 Summer</td>\n",
       "      <td>1992</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Basketball</td>\n",
       "      <td>Basketball Men's Basketball</td>\n",
       "      <td>None</td>\n",
       "      <td>24.0*180.0?80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A Lamusi</td>\n",
       "      <td>M</td>\n",
       "      <td>China</td>\n",
       "      <td>CHN</td>\n",
       "      <td>2012 Summer</td>\n",
       "      <td>2012</td>\n",
       "      <td>Summer</td>\n",
       "      <td>London</td>\n",
       "      <td>Judo</td>\n",
       "      <td>Judo Men's Extra-Lightweight</td>\n",
       "      <td>None</td>\n",
       "      <td>23.0(170.0?60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Gunnar Nielsen Aaby</td>\n",
       "      <td>M</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1920 Summer</td>\n",
       "      <td>1920</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Antwerpen</td>\n",
       "      <td>Football</td>\n",
       "      <td>Football Men's Football</td>\n",
       "      <td>None</td>\n",
       "      <td>24.0(nan?nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Edgar Lindenau Aabye</td>\n",
       "      <td>M</td>\n",
       "      <td>Denmark/Sweden</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1900 Summer</td>\n",
       "      <td>1900</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Tug-Of-War</td>\n",
       "      <td>Tug-Of-War Men's Tug-Of-War</td>\n",
       "      <td>Gold</td>\n",
       "      <td>34.0:nan?nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Christine Jacoba Aaftink</td>\n",
       "      <td>F</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>NED</td>\n",
       "      <td>1988 Winter</td>\n",
       "      <td>1988</td>\n",
       "      <td>Winter</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>Speed Skating</td>\n",
       "      <td>Speed Skating Women's 500 metres</td>\n",
       "      <td>None</td>\n",
       "      <td>21.0(185.0?82.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                      Name Sex            Team  NOC        Games  Year  \\\n",
       "0   1                 A Dijiang   M           China  CHN  1992 Summer  1992   \n",
       "1   2                  A Lamusi   M           China  CHN  2012 Summer  2012   \n",
       "2   3       Gunnar Nielsen Aaby   M         Denmark  DEN  1920 Summer  1920   \n",
       "3   4      Edgar Lindenau Aabye   M  Denmark/Sweden  DEN  1900 Summer  1900   \n",
       "4   5  Christine Jacoba Aaftink   F     Netherlands  NED  1988 Winter  1988   \n",
       "\n",
       "   Season       City          Sport                             Event Medal  \\\n",
       "0  Summer  Barcelona     Basketball       Basketball Men's Basketball  None   \n",
       "1  Summer     London           Judo      Judo Men's Extra-Lightweight  None   \n",
       "2  Summer  Antwerpen       Football           Football Men's Football  None   \n",
       "3  Summer      Paris     Tug-Of-War       Tug-Of-War Men's Tug-Of-War  Gold   \n",
       "4  Winter    Calgary  Speed Skating  Speed Skating Women's 500 metres  None   \n",
       "\n",
       "  age-height-weight  \n",
       "0   24.0*180.0?80.0  \n",
       "1   23.0(170.0?60.0  \n",
       "2      24.0(nan?nan  \n",
       "3      34.0:nan?nan  \n",
       "4   21.0(185.0?82.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('olimpiadas.parquet')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na_medal(df):\n",
    "    df['Medal'].fillna('No', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_na_transf = FunctionTransformer(\n",
    "                func=fill_na_medal,\n",
    "                validate=False  # Puedes ajustar esto seg√∫n tus necesidades\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Sport'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler_1 = Profiler(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profiler_1.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cle,pipe_cle = profiler_1.clean_data(columns = None, columns_to_split=['age-height-weight'], type_data_split = {'age-height-weight': 'float64'},\n",
    "                               duplicados=True, drop_na=True, impute_na=fill_na_transf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = OneHotEncoder(drop = 'if_binary')\n",
    "cols = ['Sex','Season','Medal'] \n",
    "#transformers = None\n",
    "#cols = None \n",
    "l_n = ['age','height','weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled,col_trans = profiler_1.scale(categorical_transformer = transformers, numeric_scaler = None, l_cat = cols, l_nums = l_n, data_clean = True)\n",
    "#data_scaled = data_scaled.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Team</th>\n",
       "      <th>NOC</th>\n",
       "      <th>Games</th>\n",
       "      <th>Year</th>\n",
       "      <th>Season</th>\n",
       "      <th>City</th>\n",
       "      <th>Sport</th>\n",
       "      <th>Event</th>\n",
       "      <th>Medal</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A Dijiang</td>\n",
       "      <td>M</td>\n",
       "      <td>China</td>\n",
       "      <td>CHN</td>\n",
       "      <td>1992 Summer</td>\n",
       "      <td>1992</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Basketball</td>\n",
       "      <td>Basketball Men's Basketball</td>\n",
       "      <td>No</td>\n",
       "      <td>24.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A Lamusi</td>\n",
       "      <td>M</td>\n",
       "      <td>China</td>\n",
       "      <td>CHN</td>\n",
       "      <td>2012 Summer</td>\n",
       "      <td>2012</td>\n",
       "      <td>Summer</td>\n",
       "      <td>London</td>\n",
       "      <td>Judo</td>\n",
       "      <td>Judo Men's Extra-Lightweight</td>\n",
       "      <td>No</td>\n",
       "      <td>23.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Christine Jacoba Aaftink</td>\n",
       "      <td>F</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>NED</td>\n",
       "      <td>1988 Winter</td>\n",
       "      <td>1988</td>\n",
       "      <td>Winter</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>Speed Skating</td>\n",
       "      <td>Speed Skating Women's 500 metres</td>\n",
       "      <td>No</td>\n",
       "      <td>21.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Christine Jacoba Aaftink</td>\n",
       "      <td>F</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>NED</td>\n",
       "      <td>1988 Winter</td>\n",
       "      <td>1988</td>\n",
       "      <td>Winter</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>Speed Skating</td>\n",
       "      <td>Speed Skating Women's 1,000 metres</td>\n",
       "      <td>No</td>\n",
       "      <td>21.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>Christine Jacoba Aaftink</td>\n",
       "      <td>F</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>NED</td>\n",
       "      <td>1992 Winter</td>\n",
       "      <td>1992</td>\n",
       "      <td>Winter</td>\n",
       "      <td>Albertville</td>\n",
       "      <td>Speed Skating</td>\n",
       "      <td>Speed Skating Women's 500 metres</td>\n",
       "      <td>No</td>\n",
       "      <td>25.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                      Name Sex         Team  NOC        Games  Year  \\\n",
       "0   1                 A Dijiang   M        China  CHN  1992 Summer  1992   \n",
       "1   2                  A Lamusi   M        China  CHN  2012 Summer  2012   \n",
       "4   5  Christine Jacoba Aaftink   F  Netherlands  NED  1988 Winter  1988   \n",
       "5   5  Christine Jacoba Aaftink   F  Netherlands  NED  1988 Winter  1988   \n",
       "6   5  Christine Jacoba Aaftink   F  Netherlands  NED  1992 Winter  1992   \n",
       "\n",
       "   Season         City          Sport                               Event  \\\n",
       "0  Summer    Barcelona     Basketball         Basketball Men's Basketball   \n",
       "1  Summer       London           Judo        Judo Men's Extra-Lightweight   \n",
       "4  Winter      Calgary  Speed Skating    Speed Skating Women's 500 metres   \n",
       "5  Winter      Calgary  Speed Skating  Speed Skating Women's 1,000 metres   \n",
       "6  Winter  Albertville  Speed Skating    Speed Skating Women's 500 metres   \n",
       "\n",
       "  Medal   age  height  weight  \n",
       "0    No  24.0   180.0    80.0  \n",
       "1    No  23.0   170.0    60.0  \n",
       "4    No  21.0   185.0    82.0  \n",
       "5    No  21.0   185.0    82.0  \n",
       "6    No  25.0   185.0    82.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler_1.summarize(clean_data = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler_1.plot_vars(['Sex','NOC','Season','Sport','Medal','age','height','weight'],clean_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cle,pipe_cle = profiler_1.clean_data(columns = None, columns_to_split=['age-height-weight'], type_data_split = {'age-height-weight': 'float64'},\n",
    "                               duplicados=True, drop_na=True, impute_na=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = None\n",
    "cols = None \n",
    "l_n = ['age','height','weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled,col_trans = profiler_1.scale(categorical_transformer = transformers, numeric_scaler = None, l_cat = cols, l_nums = l_n, data_clean = True)\n",
    "#data_scaled = data_scaled.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.465139</td>\n",
       "      <td>0.610934</td>\n",
       "      <td>0.590086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.465139</td>\n",
       "      <td>0.509499</td>\n",
       "      <td>0.438115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.465139</td>\n",
       "      <td>0.509499</td>\n",
       "      <td>0.438115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.465139</td>\n",
       "      <td>0.509499</td>\n",
       "      <td>0.438115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.465139</td>\n",
       "      <td>0.509499</td>\n",
       "      <td>0.438115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0  0.465139  0.610934  0.590086\n",
       "1  0.465139  0.509499  0.438115\n",
       "2  0.465139  0.509499  0.438115\n",
       "3  0.465139  0.509499  0.438115\n",
       "4  0.465139  0.509499  0.438115"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El numero de cluster √≥ptimos es 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;data_cleaner&#x27;,\n",
       "                 Pipeline(steps=[(&#x27;columns_spliter&#x27;,\n",
       "                                  FunctionTransformer(func=&lt;function Profiler.clean_data.&lt;locals&gt;.&lt;lambda&gt; at 0x15a34cd30&gt;)),\n",
       "                                 (&#x27;drop_duplicates&#x27;,\n",
       "                                  FunctionTransformer(func=&lt;function Profiler.clean_data.&lt;locals&gt;.&lt;lambda&gt; at 0x15cce9ee0&gt;)),\n",
       "                                 (&#x27;drop_na&#x27;,\n",
       "                                  FunctionTransformer(func=&lt;function Profiler.clean_data.&lt;locals&gt;.&lt;lambda&gt; at 0x1059fe9d0&gt;))])),\n",
       "                (&#x27;data_scaler&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;log_scale&#x27;,\n",
       "                                                                   FunctionTransformer(func=&lt;ufunc &#x27;log1p&#x27;&gt;)),\n",
       "                                                                  (&#x27;minmax_scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;age&#x27;, &#x27;height&#x27;,\n",
       "                                                   &#x27;weight&#x27;])])),\n",
       "                (&#x27;cluster_algoritm&#x27;,\n",
       "                 KMeans(n_clusters=3, n_init=22, random_state=0))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;data_cleaner&#x27;,\n",
       "                 Pipeline(steps=[(&#x27;columns_spliter&#x27;,\n",
       "                                  FunctionTransformer(func=&lt;function Profiler.clean_data.&lt;locals&gt;.&lt;lambda&gt; at 0x15a34cd30&gt;)),\n",
       "                                 (&#x27;drop_duplicates&#x27;,\n",
       "                                  FunctionTransformer(func=&lt;function Profiler.clean_data.&lt;locals&gt;.&lt;lambda&gt; at 0x15cce9ee0&gt;)),\n",
       "                                 (&#x27;drop_na&#x27;,\n",
       "                                  FunctionTransformer(func=&lt;function Profiler.clean_data.&lt;locals&gt;.&lt;lambda&gt; at 0x1059fe9d0&gt;))])),\n",
       "                (&#x27;data_scaler&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;log_scale&#x27;,\n",
       "                                                                   FunctionTransformer(func=&lt;ufunc &#x27;log1p&#x27;&gt;)),\n",
       "                                                                  (&#x27;minmax_scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;age&#x27;, &#x27;height&#x27;,\n",
       "                                                   &#x27;weight&#x27;])])),\n",
       "                (&#x27;cluster_algoritm&#x27;,\n",
       "                 KMeans(n_clusters=3, n_init=22, random_state=0))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">data_cleaner: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;columns_spliter&#x27;,\n",
       "                 FunctionTransformer(func=&lt;function Profiler.clean_data.&lt;locals&gt;.&lt;lambda&gt; at 0x15a34cd30&gt;)),\n",
       "                (&#x27;drop_duplicates&#x27;,\n",
       "                 FunctionTransformer(func=&lt;function Profiler.clean_data.&lt;locals&gt;.&lt;lambda&gt; at 0x15cce9ee0&gt;)),\n",
       "                (&#x27;drop_na&#x27;,\n",
       "                 FunctionTransformer(func=&lt;function Profiler.clean_data.&lt;locals&gt;.&lt;lambda&gt; at 0x1059fe9d0&gt;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;function Profiler.clean_data.&lt;locals&gt;.&lt;lambda&gt; at 0x15a34cd30&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;function Profiler.clean_data.&lt;locals&gt;.&lt;lambda&gt; at 0x15cce9ee0&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;function Profiler.clean_data.&lt;locals&gt;.&lt;lambda&gt; at 0x1059fe9d0&gt;)</pre></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">data_scaler: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;log_scale&#x27;,\n",
       "                                                  FunctionTransformer(func=&lt;ufunc &#x27;log1p&#x27;&gt;)),\n",
       "                                                 (&#x27;minmax_scaler&#x27;,\n",
       "                                                  MinMaxScaler())]),\n",
       "                                 [&#x27;age&#x27;, &#x27;height&#x27;, &#x27;weight&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num</label><div class=\"sk-toggleable__content\"><pre>[&#x27;age&#x27;, &#x27;height&#x27;, &#x27;weight&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;ufunc &#x27;log1p&#x27;&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=3, n_init=22, random_state=0)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('data_cleaner',\n",
       "                 Pipeline(steps=[('columns_spliter',\n",
       "                                  FunctionTransformer(func=<function Profiler.clean_data.<locals>.<lambda> at 0x15a34cd30>)),\n",
       "                                 ('drop_duplicates',\n",
       "                                  FunctionTransformer(func=<function Profiler.clean_data.<locals>.<lambda> at 0x15cce9ee0>)),\n",
       "                                 ('drop_na',\n",
       "                                  FunctionTransformer(func=<function Profiler.clean_data.<locals>.<lambda> at 0x1059fe9d0>))])),\n",
       "                ('data_scaler',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('log_scale',\n",
       "                                                                   FunctionTransformer(func=<ufunc 'log1p'>)),\n",
       "                                                                  ('minmax_scaler',\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  ['age', 'height',\n",
       "                                                   'weight'])])),\n",
       "                ('cluster_algoritm',\n",
       "                 KMeans(n_clusters=3, n_init=22, random_state=0))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiler_1.make_clusters(KMeans(n_init = 22), listo = False ,n_clusters_max = 13, clean_data = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profiler_1.detect_anomalies(OneClassSVM(gamma='auto'), clean_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;data_cleaner&#x27;,\n",
       "                 Pipeline(steps=[(&#x27;columns_spliter&#x27;,\n",
       "                                  FunctionTransformer(func=&lt;function Profiler.clean_data.&lt;locals&gt;.&lt;lambda&gt; at 0x15a34cd30&gt;)),\n",
       "                                 (&#x27;drop_duplicates&#x27;,\n",
       "                                  FunctionTransformer(func=&lt;function Profiler.clean_data.&lt;locals&gt;.&lt;lambda&gt; at 0x15cce9ee0&gt;)),\n",
       "                                 (&#x27;drop_na&#x27;,\n",
       "                                  FunctionTransformer(func=&lt;function Profiler.clean_data.&lt;locals&gt;.&lt;lambda&gt; at 0x1059fe9d0&gt;))])),\n",
       "                (&#x27;data_scaler&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;log_scale&#x27;,\n",
       "                                                                   FunctionTransformer(func=&lt;ufunc &#x27;log1p&#x27;&gt;)),\n",
       "                                                                  (&#x27;minmax_scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;age&#x27;, &#x27;height&#x27;,\n",
       "                                                   &#x27;weight&#x27;])])),\n",
       "                (&#x27;anomaly_detector&#x27;, IsolationForest(random_state=0))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;data_cleaner&#x27;,\n",
       "                 Pipeline(steps=[(&#x27;columns_spliter&#x27;,\n",
       "                                  FunctionTransformer(func=&lt;function Profiler.clean_data.&lt;locals&gt;.&lt;lambda&gt; at 0x15a34cd30&gt;)),\n",
       "                                 (&#x27;drop_duplicates&#x27;,\n",
       "                                  FunctionTransformer(func=&lt;function Profiler.clean_data.&lt;locals&gt;.&lt;lambda&gt; at 0x15cce9ee0&gt;)),\n",
       "                                 (&#x27;drop_na&#x27;,\n",
       "                                  FunctionTransformer(func=&lt;function Profiler.clean_data.&lt;locals&gt;.&lt;lambda&gt; at 0x1059fe9d0&gt;))])),\n",
       "                (&#x27;data_scaler&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;log_scale&#x27;,\n",
       "                                                                   FunctionTransformer(func=&lt;ufunc &#x27;log1p&#x27;&gt;)),\n",
       "                                                                  (&#x27;minmax_scaler&#x27;,\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  [&#x27;age&#x27;, &#x27;height&#x27;,\n",
       "                                                   &#x27;weight&#x27;])])),\n",
       "                (&#x27;anomaly_detector&#x27;, IsolationForest(random_state=0))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">data_cleaner: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;columns_spliter&#x27;,\n",
       "                 FunctionTransformer(func=&lt;function Profiler.clean_data.&lt;locals&gt;.&lt;lambda&gt; at 0x15a34cd30&gt;)),\n",
       "                (&#x27;drop_duplicates&#x27;,\n",
       "                 FunctionTransformer(func=&lt;function Profiler.clean_data.&lt;locals&gt;.&lt;lambda&gt; at 0x15cce9ee0&gt;)),\n",
       "                (&#x27;drop_na&#x27;,\n",
       "                 FunctionTransformer(func=&lt;function Profiler.clean_data.&lt;locals&gt;.&lt;lambda&gt; at 0x1059fe9d0&gt;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;function Profiler.clean_data.&lt;locals&gt;.&lt;lambda&gt; at 0x15a34cd30&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;function Profiler.clean_data.&lt;locals&gt;.&lt;lambda&gt; at 0x15cce9ee0&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;function Profiler.clean_data.&lt;locals&gt;.&lt;lambda&gt; at 0x1059fe9d0&gt;)</pre></div></div></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">data_scaler: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;log_scale&#x27;,\n",
       "                                                  FunctionTransformer(func=&lt;ufunc &#x27;log1p&#x27;&gt;)),\n",
       "                                                 (&#x27;minmax_scaler&#x27;,\n",
       "                                                  MinMaxScaler())]),\n",
       "                                 [&#x27;age&#x27;, &#x27;height&#x27;, &#x27;weight&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num</label><div class=\"sk-toggleable__content\"><pre>[&#x27;age&#x27;, &#x27;height&#x27;, &#x27;weight&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;ufunc &#x27;log1p&#x27;&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IsolationForest</label><div class=\"sk-toggleable__content\"><pre>IsolationForest(random_state=0)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('data_cleaner',\n",
       "                 Pipeline(steps=[('columns_spliter',\n",
       "                                  FunctionTransformer(func=<function Profiler.clean_data.<locals>.<lambda> at 0x15a34cd30>)),\n",
       "                                 ('drop_duplicates',\n",
       "                                  FunctionTransformer(func=<function Profiler.clean_data.<locals>.<lambda> at 0x15cce9ee0>)),\n",
       "                                 ('drop_na',\n",
       "                                  FunctionTransformer(func=<function Profiler.clean_data.<locals>.<lambda> at 0x1059fe9d0>))])),\n",
       "                ('data_scaler',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('log_scale',\n",
       "                                                                   FunctionTransformer(func=<ufunc 'log1p'>)),\n",
       "                                                                  ('minmax_scaler',\n",
       "                                                                   MinMaxScaler())]),\n",
       "                                                  ['age', 'height',\n",
       "                                                   'weight'])])),\n",
       "                ('anomaly_detector', IsolationForest(random_state=0))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiler_1.detect_anomalies(IsolationForest(random_state = 0), clean_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisis de clusters\n",
    "path = os.path.join(profiler_1.output_folder, \"clusters\")\n",
    "path = os.path.join(path, 'data_clusters.csv')\n",
    "df_cluster = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = df_cluster.groupby(['Cluster']).agg({\"age\":['mean','std','max','min'],\n",
    "                                               \"height\":['mean','std','max','min'],\n",
    "                                               \"weight\":['mean','std','max','min'],\n",
    "                                               \"Sex\":['count']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group_sex = df_cluster.groupby(['Cluster','Sex'])['Sex'].count().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Medal</th>\n",
       "      <th>Bronze</th>\n",
       "      <th>Gold</th>\n",
       "      <th>Silver</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3954</td>\n",
       "      <td>4236</td>\n",
       "      <td>3899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2897</td>\n",
       "      <td>2705</td>\n",
       "      <td>2849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3297</td>\n",
       "      <td>3226</td>\n",
       "      <td>3118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Medal    Bronze  Gold  Silver\n",
       "Cluster                      \n",
       "0          3954  4236    3899\n",
       "1          2897  2705    2849\n",
       "2          3297  3226    3118"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group_medal = df_cluster.groupby(['Cluster','Medal'])['Medal'].count().unstack()\n",
    "df_group_medal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group['F']=df_group_sex.loc[:,'F']\n",
    "df_group['M']=df_group_sex.loc[:,'M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">age</th>\n",
       "      <th colspan=\"4\" halign=\"left\">height</th>\n",
       "      <th colspan=\"4\" halign=\"left\">weight</th>\n",
       "      <th>Sex</th>\n",
       "      <th>F</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>count</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.412772</td>\n",
       "      <td>3.496551</td>\n",
       "      <td>41.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>187.304078</td>\n",
       "      <td>7.089425</td>\n",
       "      <td>223.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>86.194019</td>\n",
       "      <td>12.137964</td>\n",
       "      <td>182.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>12089</td>\n",
       "      <td>1314</td>\n",
       "      <td>10775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.509999</td>\n",
       "      <td>4.755637</td>\n",
       "      <td>66.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>175.477222</td>\n",
       "      <td>6.212246</td>\n",
       "      <td>198.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>71.355875</td>\n",
       "      <td>9.233480</td>\n",
       "      <td>118.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>8451</td>\n",
       "      <td>3055</td>\n",
       "      <td>5396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.249455</td>\n",
       "      <td>3.261606</td>\n",
       "      <td>34.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>167.425267</td>\n",
       "      <td>7.154551</td>\n",
       "      <td>185.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>60.255990</td>\n",
       "      <td>8.004746</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>9641</td>\n",
       "      <td>5981</td>\n",
       "      <td>3660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               age                            height                          \\\n",
       "              mean       std   max   min        mean       std    max    min   \n",
       "Cluster                                                                        \n",
       "0        24.412772  3.496551  41.0  15.0  187.304078  7.089425  223.0  167.0   \n",
       "1        30.509999  4.755637  66.0  24.0  175.477222  6.212246  198.0  147.0   \n",
       "2        22.249455  3.261606  34.0  13.0  167.425267  7.154551  185.0  136.0   \n",
       "\n",
       "            weight                            Sex     F      M  \n",
       "              mean        std    max   min  count               \n",
       "Cluster                                                         \n",
       "0        86.194019  12.137964  182.0  59.0  12089  1314  10775  \n",
       "1        71.355875   9.233480  118.0  41.0   8451  3055   5396  \n",
       "2        60.255990   8.004746   95.0  28.0   9641  5981   3660  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Age\n",
      "Age Group Sex     \n",
      "20-30     F      1\n",
      "          M      1\n",
      "30-40     F      1\n",
      "          M      1\n",
      "40-50     F      0\n",
      "          M      2\n",
      "50-60     F      2\n",
      "          M      0\n",
      "60-70     F      0\n",
      "          M      0\n"
     ]
    }
   ],
   "source": [
    "# Crear un DataFrame de ejemplo\n",
    "data = {'Age': [25, 30, 35, 40, 45, 50, 55, 60],\n",
    "        'Sex': ['M', 'F', 'M', 'F', 'M', 'M', 'F', 'F']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Definir los rangos de edad\n",
    "bins = [20, 30, 40, 50, 60, 70]\n",
    "\n",
    "# Crear una columna 'Age Group' usando pd.cut\n",
    "df['Age Group'] = pd.cut(df['Age'], bins=bins, labels=['20-30', '30-40', '40-50', '50-60', '60-70'])\n",
    "\n",
    "# Realizar el groupby y contar el n√∫mero de hombres y mujeres en cada grupo\n",
    "result = df.groupby(['Age Group', 'Sex']).size().unstack()\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profiler_1.clearGarbage()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
